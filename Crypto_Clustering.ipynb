{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl in c:\\users\\stona\\onedrive\\documents\\python scripts\\lib\\site-packages (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade threadpoolctl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and dependencies\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a Pandas DataFrame\n",
    "df_market_data = pd.read_csv(\n",
    "    \"Resources/crypto_market_data.csv\",\n",
    "    index_col=\"coin_id\")\n",
    "\n",
    "# Display sample data\n",
    "df_market_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "df_market_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your data to see what's in your DataFrame\n",
    "df_market_data.hvplot.line(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    rot=90\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `StandardScaler()` module from scikit-learn to normalize the data from the CSV file\n",
    "\n",
    "# Load data from a CSV file\n",
    "data = pd.read_csv('Resources/crypto_market_data.csv')\n",
    "\n",
    "# Select only numeric columns for scaling\n",
    "numeric_data = data.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numeric data\n",
    "normalized_data = scaler.fit_transform(numeric_data)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=numeric_data.columns)\n",
    "\n",
    "# Optionally, merge normalized data with non-numeric columns\n",
    "non_numeric_data = data.select_dtypes(exclude=['float64', 'int64'])\n",
    "final_df = pd.concat([normalized_df, non_numeric_data], axis=1)\n",
    "\n",
    "# Optionally, save the normalized data to a new CSV file\n",
    "final_df.to_csv('normalized_crypto_market_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the scaled data\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=numeric_data.columns)\n",
    "\n",
    "# Copy the crypto names from the original DataFrame\n",
    "crypto_names = data['coin_id']\n",
    "\n",
    "# Set the coin_id column as index\n",
    "normalized_df['coin_id'] = crypto_names\n",
    "normalized_df.set_index('coin_id', inplace=True)\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "print(normalized_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Best Value for k Using the Original Scaled DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the number of k-values from 1 to 11\n",
    "k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "print(k_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the inertia values\n",
    "inertia = []\n",
    "\n",
    "# Reduce the number of clusters to 3 (or any number <= 5)\n",
    "k = 3\n",
    "\n",
    "# Create a for loop to compute the inertia with each possible value of k\n",
    "# Inside the loop: \n",
    "for k in range(1, 11):\n",
    "    # 1. Create a KMeans model using the loop counter for the n_clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    \n",
    "    # 2. Fit the model to the data using `normalized_df`\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(normalized_df)\n",
    "    \n",
    "    # 3. Append the model.inertia_ to the inertia list\n",
    "    inertia = []\n",
    "    inertia.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the data to plot the Elbow curve\n",
    "elbow_data = {'k': range(1, len(inertia) + 1), 'inertia': inertia}\n",
    "\n",
    "# Create a DataFrame with the data to plot the Elbow curve\n",
    "elbow_df = pd.DataFrame(elbow_data)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(elbow_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a line chart with all the inertia values computed with\n",
    "# the different values of k to visually identify the optimal value for k.\n",
    "\n",
    "plt.plot(elbow_df['k'], elbow_df['inertia'], marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the following question: \n",
    "\n",
    "**Question:** What is the best value for `k`?\n",
    "\n",
    "**Answer:** \n",
    "The best value for k is typically identified using the Elbow method. This involves plotting the inertia (sum of squared distances from each point to its assigned cluster center) for different values of k, and then visually inspecting the plot to find the \"elbow point.\" The elbow point is where the rate of decrease sharply slows down, indicating diminishing returns for increasing k.\n",
    "\n",
    "In your case, you can plot the inertia values using the code provided and look for the point where the plot forms an elbow. The optimal k is generally the value at this elbow point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Cryptocurrencies with K-means Using the Original Scaled DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimal number of clusters (k) determined from the Elbow curve\n",
    "best_k = 4  \n",
    "\n",
    "# Initialize the K-Means model using the best value for k\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the K-Means model using the scaled DataFrame\n",
    "kmeans.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the clusters to group the cryptocurrencies using the scaled DataFrame\n",
    "cluster_labels = kmeans.predict(X_scaled)\n",
    "\n",
    "# Print the resulting array of cluster values.\n",
    "print(\"Cluster labels:\", cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the scaled DataFrame\n",
    "scaled_df_copy = scaled_df.copy()\n",
    "\n",
    "print(scaled_df_copy.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to the copy of the scaled DataFrame with the predicted clusters\n",
    "scaled_df_copy['Cluster'] = cluster_labels\n",
    "\n",
    "# Display the copy of the scaled DataFrame\n",
    "print(scaled_df_copy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot using hvPlot by setting\n",
    "scatter_plot = scaled_df_copy.hvplot.scatter(\n",
    "    x=\"price_change_percentage_24h\", \n",
    "    y=\"price_change_percentage_7d\",\n",
    "    c=\"Cluster\",  # Color the graph points with the labels found using K-Means and\n",
    "    hover_cols=[\"crypto_name\"]  # add the crypto name in the `hover_cols` parameter to identify\n",
    ") \n",
    "# `x=\"price_change_percentage_24h\"` and `y=\"price_change_percentage_7d\"`.\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# the cryptocurrency represented by each data point.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Clusters with Principal Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA model instance and set `n_components=3`.\n",
    "pca = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the PCA model with `fit_transform` to reduce the original scaled DataFrame\n",
    "pca_data = pca.fit_transform(X_scaled)\n",
    "\n",
    "# down to three principal components.\n",
    "pca_df = pd.DataFrame(\n",
    "    data=pca_data, \n",
    "    columns=['PC1', 'PC2', 'PC3']  # Naming the principal components\n",
    ")\n",
    "\n",
    "# View the scaled PCA data\n",
    "print(pca_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the explained variance to determine how much information\n",
    "# can be attributed to each principal component.\n",
    "explained_variance = pca.explained_variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the following question: \n",
    "\n",
    "**Question:** What is the total explained variance of the three principal components?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the PCA data.\n",
    "pca_df = pd.DataFrame(pca_data)\n",
    "\n",
    "# Copy the crypto names from the original scaled DataFrame\n",
    "pca_df[\"coin_id\"] = scaled_df[\"coin_id\"]\n",
    "\n",
    "# Set the coin_id column as index\n",
    "pca_df.set_index(\"coin_id\", inplace=True)\n",
    "\n",
    "# Display the scaled PCA DataFrame\n",
    "print(pca_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Best Value for k Using the Scaled PCA DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the number of k-values from 1 to 11\n",
    "k_values = list(range(1, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the inertia values\n",
    "inertia = []\n",
    "\n",
    "# Create a for loop to compute the inertia with each possible value of k\n",
    "# Inside the loop:\n",
    "for k in range(1, 12):\n",
    "    \n",
    "# 1. Create a KMeans model using the loop counter for the n_clusters\n",
    "     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    \n",
    "# 2. Fit the model to the data using `df_market_data_pca`\n",
    "kmeans.fit(df_market_data_pca)\n",
    "\n",
    "# 3. Append the model.inertia_ to the inertia list\n",
    "   inertia.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the data to plot the Elbow curve\n",
    "elbow_data = {\n",
    "    \"k\": k_values,\n",
    "    \"inertia\": inertia\n",
    "}\n",
    "\n",
    "# Create a DataFrame with the data to plot the Elbow curve\n",
    "elbow_df = pd.DataFrame(elbow_data)\n",
    "\n",
    "print(elbow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a line chart with all the inertia values computed with\n",
    "# the different values of k to visually identify the optimal value for k.\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia, marker='o', linestyle='-', color='b')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the following questions: \n",
    "\n",
    "* **Question:** What is the best value for `k` when using the PCA data?\n",
    "\n",
    "  * **Answer:**\n",
    "\n",
    "\n",
    "* **Question:** Does it differ from the best k value found using the original data?\n",
    "\n",
    "  * **Answer:** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Cryptocurrencies with K-means Using the Scaled PCA DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the K-Means model using the best value for k\n",
    "kmeans_model = KMeans(n_clusters=optimal_k, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the K-Means model using the PCA data\n",
    "kmeans_model.fit(df_market_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the clusters to group the cryptocurrencies using the scaled PCA DataFrame\n",
    "predicted_clusters = kmeans_model.predict(df_market_data_pca)\n",
    "\n",
    "# Print the resulting array of cluster values.\n",
    "print(predicted_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the scaled PCA DataFrame\n",
    "pca_scaled_copy = df_market_data_pca.copy()\n",
    "\n",
    "# Add a new column to the copy of the PCA DataFrame with the predicted clusters\n",
    "pca_scaled_copy[\"Cluster\"] = predicted_clusters\n",
    "\n",
    "# Display the copy of the scaled PCA DataFrame\n",
    "print(pca_scaled_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot using hvPlot by setting\n",
    "scatter_plot = pca_scaled_copy.hvplot.scatter(\n",
    "    x=\"PC1\", \n",
    "    y=\"PC2\", \n",
    "    c=\"Cluster\",  # Color the graph points with the labels found using K-Means and\n",
    "    hover_cols=[\"coin_id\"],  # add the crypto name in the `hover_cols` parameter to identify the cryptocurrency represented by each data point.\n",
    "    title=\"Cryptocurrency Clusters\"\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "scatter_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and Compare the Results\n",
    "\n",
    "In this section, you will visually analyze the cluster analysis results by contrasting the outcome with and without using the optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite plot to contrast the Elbow curves\n",
    "or dataset_name, data in datasets.items():\n",
    "    plt.plot(data[\"k_values\"], data[\"inertia\"], marker='o', label=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite plot to contrast the clusters\n",
    "for ax, (dataset_name, data) in zip(axes, datasets.items()):\n",
    "    ax.scatter(\n",
    "        data[\"PC1\"], \n",
    "        data[\"PC2\"], \n",
    "        c=data[\"Cluster\"], \n",
    "        cmap=\"viridis\", \n",
    "        s=100, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    ax.set_title(dataset_name, fontsize=12)\n",
    "    ax.set_xlabel(\"PC1\", fontsize=10)\n",
    "    ax.set_ylabel(\"PC2\", fontsize=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the following question: \n",
    "\n",
    "  * **Question:** After visually analyzing the cluster analysis results, what is the impact of using fewer features to cluster the data using K-Means?\n",
    "\n",
    "  * **Answer:** The impact is both positive and negative. The positive impact includes (a)Improved Performance, (b) reduced risk of overfitting, and (c) simplified visualiztion. On the other hand, the negative impact includes (a) loss of Information, (b) over simplification of the data, and (c) bias in results.\n",
    "\n",
    "The impact also includes a trade off which involves balancing interpretability and computational efficiency against the need to retain meaningful patterns in the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
